{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "9d783555-cfa2-4272-b9c7-02fe5ebddc04",
   "metadata": {},
   "source": [
    "### Streaming Index Pipeline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "30d599c1-8d36-44f2-a37a-c9d578b45378",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Skip if you already have Kafka\n",
    "\n",
    "# # pull docker images\n",
    "# ! docker pull apache/kafka:3.8.0\n",
    "\n",
    "# Start Kafka and Flink containers\n",
    "# ! docker run -d --name kafka -p 9092:9092 apache/kafka:3.8.0\n",
    "\n",
    "# # Install kafka-python client\n",
    "# ! pip3 install kafka-python\n",
    "\n",
    "# # Download spark-sql-kafka jar from https://mvnrepository.com/artifact/org.apache.spark/spark-sql-kafka-0-10_2.12/3.4.3\n",
    "# # Download kafka-client jar from https://mvnrepository.com/artifact/org.apache.kafka/kafka-clients/3.3.2\n",
    "# export PYSPARK_SUBMIT_ARGS='--jars /opt/spark-3.4.3/jars/spark-sql-kafka-0-10_2.12-3.4.3.jar,/opt/spark-3.4.3/jars/kafka-clients-3.3.2.jar pyspark-shell'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "8b189134-995e-4dfb-bbfe-09cd3559c4f2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Message produced with the offset: 1\n"
     ]
    }
   ],
   "source": [
    "import json\n",
    "import datetime\n",
    "\n",
    "import sys\n",
    "\n",
    "if sys.version_info >= (3, 12, 0):\n",
    "    import six\n",
    "    sys.modules['kafka.vendor.six.moves'] = six.moves\n",
    "\n",
    "from kafka import KafkaProducer\n",
    "\n",
    "def on_success(metadata):\n",
    "    print(f\"Message produced with the offset: {metadata.offset}\")\n",
    "\n",
    "def on_error(error):\n",
    "    print(f\"An error occurred while publishing the message. {error}\")\n",
    "\n",
    "producer = KafkaProducer(\n",
    "    bootstrap_servers='localhost:9092',\n",
    "    value_serializer=lambda v: json.dumps(v).encode('utf-8')\n",
    ")\n",
    "\n",
    "message = {\n",
    "    \"COMMENT_ID\" : \"z123std54m2ozht10232efr5svb4vh0au04\",\n",
    "    \"CONTENT\": \"damn nvm what I said\"\n",
    "}\n",
    "\n",
    "# Send updates to 'youtube_update' topic\n",
    "future = producer.send(\"youtube_update\", message)\n",
    "future.add_callback(on_success)\n",
    "future.add_errback(on_error)\n",
    "\n",
    "# Ensure all messages are sent before exiting\n",
    "producer.flush()\n",
    "producer.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "d5ba5261-8826-47fe-a18d-4c125a5ba813",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting default log level to \"WARN\".\n",
      "To adjust logging level use sc.setLogLevel(newLevel). For SparkR, use setLogLevel(newLevel).\n",
      "24/10/09 23:06:45 WARN NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----------+\n",
      "|        _c1|\n",
      "+-----------+\n",
      "|         69|\n",
      "|        ass|\n",
      "|       fuck|\n",
      "|       fuck|\n",
      "|       fuck|\n",
      "|        ass|\n",
      "|        sex|\n",
      "|        sex|\n",
      "|     orgasm|\n",
      "|     orgasm|\n",
      "|ejaculation|\n",
      "|       arse|\n",
      "|       arse|\n",
      "|       arse|\n",
      "|   foreskin|\n",
      "|       shit|\n",
      "|       shit|\n",
      "|      skank|\n",
      "|        ass|\n",
      "|    abraham|\n",
      "+-----------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Download profane words dataset: https://www.kaggle.com/datasets/konradb/profanities-in-english-collection\n",
    "path = \"profanity_en.csv\"\n",
    "\n",
    "import pyspark\n",
    "from pyspark.sql import SparkSession\n",
    "\n",
    "conf = pyspark.SparkConf()\n",
    "spark = SparkSession.builder.appName(\"indexer\").config(conf=conf).getOrCreate()\n",
    "\n",
    "df = spark.read.csv(path)\n",
    "profane_df = df.select(df._c1)\n",
    "profane_df = profane_df.filter(df._c1 != 'canonical_form_1')\n",
    "profane_df.show()\n",
    "\n",
    "profane_set = set(profane_df.select(profane_df._c1).rdd.flatMap(lambda x: x).collect())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "c3587bab-cf8c-4fbf-9f0d-f18a65ad3570",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Read from kafka and mask profane words from content\n",
    "from pyspark.sql.functions import from_json\n",
    "from pyspark.sql.types import StructType, StructField, StringType\n",
    "\n",
    "schema = StructType([\n",
    "    StructField(\"COMMENT_ID\", StringType(), True),\n",
    "    StructField(\"CONTENT\", StringType(), True)\n",
    "])\n",
    "\n",
    "kafka_stream = spark.readStream \\\n",
    "    .format(\"kafka\") \\\n",
    "    .option(\"kafka.bootstrap.servers\", \"localhost:9092\") \\\n",
    "    .option(\"subscribe\", \"youtube_update\") \\\n",
    "    .option(\"startingOffsets\", \"latest\") \\\n",
    "    .option(\"failOnDataLoss\", \"false\") \\\n",
    "    .load()\n",
    "\n",
    "data = {\n",
    "    \"COMMENT_ID\" : \"z123std54m2ozht10232efr5svb4vh0au04\",\n",
    "    \"CONTENT\": \"damn nvm what I said\"\n",
    "}\n",
    "\n",
    "json_stream = kafka_stream.selectExpr(\"CAST(value AS STRING)\") \\\n",
    "    .select(from_json(\"value\", schema).alias(\"data\")) \\\n",
    "    .select(\"data.*\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cf8e1a06-5d69-4052-b2f8-c114ad738ffd",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "24/10/09 23:07:34 WARN ResolveWriteToStream: spark.sql.adaptive.enabled is not supported in streaming DataFrames/Datasets and will be disabled.\n",
      "24/10/09 23:07:34 WARN KafkaMicroBatchStream: Partition youtube_update-0's offset was changed from 11 to 1, some data may have been missed. \n",
      "Some data may have been lost because they are not available in Kafka any more; either the\n",
      " data was aged out by Kafka or the topic may have been deleted before all the data in the\n",
      " topic was processed. If you want your streaming query to fail on such cases, set the source\n",
      " option \"failOnDataLoss\" to \"true\".\n",
      "    \n",
      "24/10/09 23:07:34 WARN KafkaMicroBatchStream: Partition youtube_update-0's offset was changed from 11 to 1, some data may have been missed. \n",
      "Some data may have been lost because they are not available in Kafka any more; either the\n",
      " data was aged out by Kafka or the topic may have been deleted before all the data in the\n",
      " topic was processed. If you want your streaming query to fail on such cases, set the source\n",
      " option \"failOnDataLoss\" to \"true\".\n",
      "    \n",
      "24/10/09 23:07:34 WARN KafkaMicroBatchStream: Partition youtube_update-0's offset was changed from 11 to 1, some data may have been missed. \n",
      "Some data may have been lost because they are not available in Kafka any more; either the\n",
      " data was aged out by Kafka or the topic may have been deleted before all the data in the\n",
      " topic was processed. If you want your streaming query to fail on such cases, set the source\n",
      " option \"failOnDataLoss\" to \"true\".\n",
      "    \n",
      "24/10/09 23:07:34 WARN KafkaMicroBatchStream: Partition youtube_update-0's offset was changed from 11 to 1, some data may have been missed. \n",
      "Some data may have been lost because they are not available in Kafka any more; either the\n",
      " data was aged out by Kafka or the topic may have been deleted before all the data in the\n",
      " topic was processed. If you want your streaming query to fail on such cases, set the source\n",
      " option \"failOnDataLoss\" to \"true\".\n",
      "    \n",
      "24/10/09 23:07:34 WARN AdminClientConfig: These configurations '[key.deserializer, value.deserializer, enable.auto.commit, max.poll.records, auto.offset.reset]' were supplied but are not used yet.\n",
      "24/10/09 23:07:35 WARN KafkaDataConsumer: KafkaDataConsumer is not running in UninterruptibleThread. It may hang when KafkaDataConsumer's methods are interrupted because of KAFKA-1894\n",
      "24/10/09 23:08:35 ERROR NetworkClient: Node [172.24.0.4:9200] failed (org.elasticsearch.hadoop.thirdparty.apache.commons.httpclient.ConnectTimeoutException: The host did not accept the connection within timeout of 60000 ms); selected next node [127.0.0.1:9200]\n",
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "# Update the DataFrame to an Elasticsearch index\n",
    "from pyspark.sql.functions import UserDefinedFunction\n",
    "from pyspark.sql.types import StringType\n",
    "\n",
    "es_conf = {\n",
    "    \"es.nodes.discovery\": \"false\",\n",
    "    \"es.nodes.data.only\": \"false\",\n",
    "    \"es.net.http.auth.user\": \"elastic\",\n",
    "    \"es.net.http.auth.pass\": \"password\",\n",
    "    \"es.nodes\": \"http://127.0.0.1\",\n",
    "    \"es.port\": \"9200\",\n",
    "    \"es.mapping.id\": \"COMMENT_ID\",\n",
    "    \"es.mapping.exclude\": \"COMMENT_ID\",\n",
    "    \"es.write.operation\": \"update\",\n",
    "    \"checkpointLocation\": \"/tmp/\",\n",
    "    \"es.spark.sql.streaming.sink.log.enabled\": \"false\"\n",
    "}\n",
    "\n",
    "def mask_profane_word(s):\n",
    "    words = s.split()\n",
    "    tokens = [t if t not in profane_set else '****' for t in words]\n",
    "    return ' '.join(tokens)\n",
    "\n",
    "def mask_profane_word_df(df, epoch_id):\n",
    "    name = 'CONTENT'\n",
    "    udf = UserDefinedFunction(lambda x: mask_profane_word(x), StringType())\n",
    "    new_df = df.select(*[udf(column).alias(name) if column == name else column for column in df.columns])\n",
    "    \n",
    "    new_df.write.mode(\"append\") \\\n",
    "        .format('org.elasticsearch.spark.sql') \\\n",
    "        .options(**es_conf) \\\n",
    "        .save('youtube')\n",
    "   \n",
    "query = json_stream.writeStream \\\n",
    "        .outputMode(\"append\") \\\n",
    "        .foreachBatch(mask_profane_word_df) \\\n",
    "        .options(**es_conf) \\\n",
    "        .start(\"youtube\")\n",
    "\n",
    "query.awaitTermination()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c285f7ff-8d6d-4934-864b-81cddb5a3b6a",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4400fc6a-2d89-4e85-9ace-329f623da636",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv-Python-3.12",
   "language": "python",
   "name": "venv-python-3.12"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
